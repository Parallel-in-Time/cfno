sample:
  inSize: 1
  outStep: 10
  inStep: 500

data:
  outType: update
  outScaling: 100
  dataFile: dataset_256x64_Ra1e7_dt1e-2_update.h5
  trainRatio: 0.8
  seed: null
  batchSize: 20
  xStep: 1
  yStep: 1

model:
  da: 4
  du: 4
  dv: 16
  kX: 12
  kY: 12
  nLayers: 2
  forceFFT: false
  bias: false
  scaling_layers: 4
  use_prechannel_mlp: true
  channel_mlp_expansion: 4
  use_fno_skip_connection: false
  fno_skip_type: soft-gating
  use_postfnochannel_mlp: false
  channel_mlp_skip_type: 'soft-gating'
  get_subdomain_output: false             # set it to false during training
  iXBeg: 5
  iYBeg: 12
  iXEnd: 30
  iYEnd: 60

optim:
  name: adam
  lr: 0.0001
  weight_decay: 1.0e-5

lr_scheduler:
  scheduler: CosAnnealingLR
  T_max: 50000

train:
  trainDir: run24_dt1e_2_update
  nEpochs: 11500
  checkpoint: model.pt
  lossesFile: losses.txt
  saveEvery: 100
  savePermanent: false
  noTensorboard: false
  logPrint: false

eval:
  iSimu: 8
  imgExt: png
  evalDir: evalDir